{"paragraphs":[{"text":"%sh\nURL=`echo aHR0cHM6Ly9zMy1ldS13ZXN0LTEuYW1hem9uYXdzLmNvbS9pb3QtZGVtby10ZWxlbWV0cnktMS9jYXJfdGVsZW1ldHJ5Lmpzb24udGFyLmd6Cg== | base64 --decode`\ncurl -o /root/car_telemetry.json.tar.gz $URL","user":"anonymous","dateUpdated":"2017-12-20T23:18:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 19.5M  100 19.5M    0     0  58.0M      0 --:--:-- --:--:-- --:--:-- 58.0M\n"}]},"apps":[],"jobName":"paragraph_1513808203333_-1567720878","id":"20171220-221643_1830922616","dateCreated":"2017-12-20T22:16:43+0000","dateStarted":"2017-12-20T23:18:31+0000","dateFinished":"2017-12-20T23:18:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:17845"},{"text":"%sh\nls -lah /","user":"anonymous","dateUpdated":"2017-12-20T23:25:42+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":159,"optionOpen":false}}},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513808885324_1681296405","id":"20171220-222805_342469011","dateCreated":"2017-12-20T22:28:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:17846","dateFinished":"2017-12-20T23:25:42+0000","dateStarted":"2017-12-20T23:25:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"total 544M\ndrwxr-xr-x   1 root    root  4.0K Dec 20 23:19 .\ndrwxr-xr-x   1 root    root  4.0K Dec 20 23:19 ..\n-rwxr-xr-x   1 root    root     0 Dec 19 08:55 .dockerenv\ndrwxr-xr-x   1 3831296 audio 4.0K Dec 16 20:02 HANA_DP_AGENT_20_LIN_X86_64\ndrwxr-xr-x   1 root    root  4.0K Dec 16 19:56 bin\ndrwxr-xr-x   2 root    root  4.0K Apr 12  2016 boot\n-rw-r--r--   1    1024  1005 543M Dec 20 23:13 car_telemetry.json\ndrwxr-xr-x   5 root    root   340 Dec 20 20:33 dev\ndrwxr-xr-x   1 root    root  4.0K Dec 20 20:53 etc\n-rw-r--r--   1 root    root  1.6M Dec 16 19:56 get-pip.py\ndrwxr-xr-x   1 root    root  4.0K Dec 16 20:00 home\ndrwxr-xr-x   1 root    root  4.0K Sep 13  2015 lib\ndrwxr-xr-x   2 root    root  4.0K Nov 14 13:49 lib64\ndrwxr-xr-x   2 root    root  4.0K Nov 14 13:48 media\ndrwxr-xr-x   2 root    root  4.0K Nov 14 13:48 mnt\ndrwxr-xr-x   1 root    root  4.0K Dec 19 09:09 opt\ndr-xr-xr-x 441 root    root     0 Dec 20 20:33 proc\ndrwx------   1 root    root  4.0K Dec 20 23:16 root\ndrwxr-xr-x   1 root    root  4.0K Dec 19 09:08 run\ndrwxr-xr-x   1 root    root  4.0K Dec 16 19:56 sbin\ndrwxr-xr-x   2 root    root  4.0K Nov 14 13:48 srv\ndr-xr-xr-x  13 root    root     0 Dec 20 20:33 sys\ndrwxrwxrwt   1 root    root  4.0K Dec 20 22:55 tmp\ndrwxr-xr-x   1 root    root  4.0K Dec 16 20:00 usr\ndrwxr-xr-x   1 root    root  4.0K Nov 14 13:49 var\n"}]}},{"text":"%sh\ntar -xvzf /root/car_telemetry.json.tar.gz ","user":"anonymous","dateUpdated":"2017-12-20T23:20:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513809078218_-1265051020","id":"20171220-223118_1109214306","dateCreated":"2017-12-20T22:31:18+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18097","dateFinished":"2017-12-20T23:20:14+0000","dateStarted":"2017-12-20T23:20:12+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"tar: /root: Not found in archive\ntar: Exiting with failure status due to previous errors\n"},{"type":"TEXT","data":"ExitValue: 2"}]}},{"text":"%sh\nhadoop fs -mkdir /car_telemetrydata\nhadoop fs -put /car_telemetry.json /car_telemetrydata","user":"anonymous","dateUpdated":"2017-12-20T23:26:10+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513809155413_1053369231","id":"20171220-223235_1885399224","dateCreated":"2017-12-20T22:32:35+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18181","dateFinished":"2017-12-20T23:26:14+0000","dateStarted":"2017-12-20T23:26:10+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"mkdir: `/car_telemetrydata': File exists\n17/12/20 23:26:13 WARN hdfs.DFSClient: DataStreamer Exception\norg.apache.hadoop.ipc.RemoteException(java.io.IOException): File /car_telemetrydata/car_telemetry.json._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n\tat org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1628)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3121)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3045)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:725)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:493)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2213)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1413)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n\tat com.sun.proxy.$Proxy11.addBlock(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1588)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1373)\n\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)\nput: File /car_telemetrydata/car_telemetry.json._COPYING_ could only be replicated to 0 nodes instead of minReplication (=1).  There are 0 datanode(s) running and no node(s) are excluded in this operation.\n"},{"type":"TEXT","data":"ExitValue: 1"}]}},{"text":"%sh\nps xau ","user":"anonymous","dateUpdated":"2017-12-20T22:59:45+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513809479429_386687519","id":"20171220-223759_1801859758","dateCreated":"2017-12-20T22:37:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18298","dateFinished":"2017-12-20T22:59:45+0000","dateStarted":"2017-12-20T22:59:45+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"USER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot          1  0.0  0.0  18172  3056 ?        Ss   20:33   0:00 bash /etc/bootstrap.sh --service resourcemanager --service nodemanager --service namenode --service datanode --service dpagent --service copydatasource --service metastore --service thriftserver --service livyserver --service zeppelin --service copys3\nroot         95  0.2  0.4 2851024 515056 ?      Sl   20:33   0:21 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dproc_namenode -Xmx1000m -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,console -Djava.library.path=/usr/local/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Stack=true -Dhadoop.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=hadoop--namenode-spark-hdfs-adapter.log -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.id.str= -Dhadoop.root.logger=INFO,RFA -Djava.library.path=/usr/local/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS -Dhdfs.audit.logger=INFO,NullAppender -Dhadoop.security.logger=INFO,RFAS org.apache.hadoop.hdfs.server.namenode.NameNode\nroot        185  0.5  0.3 3017608 468520 ?      Sl   20:33   0:52 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dproc_resourcemanager -Xmx1000m -Dhadoop.log.dir=/usr/local/hadoop/logs -Dyarn.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=yarn--resourcemanager-spark-hdfs-adapter.log -Dyarn.log.file=yarn--resourcemanager-spark-hdfs-adapter.log -Dyarn.home.dir= -Dyarn.id.str= -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/local/hadoop/lib/native -Dyarn.policy.file=hadoop-policy.xml -Dhadoop.log.dir=/usr/local/hadoop/logs -Dyarn.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=yarn--resourcemanager-spark-hdfs-adapter.log -Dyarn.log.file=yarn--resourcemanager-spark-hdfs-adapter.log -Dyarn.home.dir=/usr/local/hadoop -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/local/hadoop/lib/native -classpath /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/local/hadoop/share/hadoop/tools/lib/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/local/hadoop/share/hadoop/tools/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/etc/hadoop/rm-config/log4j.properties org.apache.hadoop.yarn.server.resourcemanager.ResourceManager\nroot        781  0.4  0.4 2902796 513576 ?      Sl   20:33   0:36 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dproc_nodemanager -Xmx1000m -Dhadoop.log.dir=/usr/local/hadoop/logs -Dyarn.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=yarn--nodemanager-spark-hdfs-adapter.log -Dyarn.log.file=yarn--nodemanager-spark-hdfs-adapter.log -Dyarn.home.dir= -Dyarn.id.str= -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/local/hadoop/lib/native -Dyarn.policy.file=hadoop-policy.xml -server -Dhadoop.log.dir=/usr/local/hadoop/logs -Dyarn.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=yarn--nodemanager-spark-hdfs-adapter.log -Dyarn.log.file=yarn--nodemanager-spark-hdfs-adapter.log -Dyarn.home.dir=/usr/local/hadoop -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.root.logger=INFO,RFA -Dyarn.root.logger=INFO,RFA -Djava.library.path=/usr/local/hadoop/lib/native -classpath /usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/local/hadoop/share/hadoop/tools/lib/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/lib/jvm/java-8-openjdk-amd64/lib/tools.jar:/usr/local/hadoop/share/hadoop/tools/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/etc/hadoop/nm-config/log4j.properties org.apache.hadoop.yarn.server.nodemanager.NodeManager\nroot       1837  0.0  0.0   4412   692 ?        S    20:34   0:00 tail -f /usr/sap/dataprovagent/log/framework.trc\nroot       2024  0.0  0.2 37349736 309368 ?     Sl   20:35   0:07 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -cp /usr/local/livy/jars/*:/usr/local/livy/conf:/usr/local/hadoop/etc/hadoop: org.apache.livy.server.LivyServer\ndpagent    2278  0.0  0.0 736040 21528 ?        S    20:35   0:00 ./DPAgentService -I/usr/sap/dataprovagent\ndpagent    2280  0.2  0.2 9537792 252388 ?      Sl   20:35   0:20 /usr/sap/dataprovagent/dpagent\nroot       2515  0.4  0.4 9778112 611172 ?      Sl   20:35   0:42 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding=UTF-8 -Xms1024m -Xmx1024m -XX:MaxPermSize=512m -Dlog4j.configuration=file:///opt/zeppelin-0.7.3/conf/log4j.properties -Dzeppelin.log.file=/opt/zeppelin-0.7.3/logs/zeppelin--spark-hdfs-adapter.log -cp ::/opt/zeppelin-0.7.3/lib/interpreter/*:/opt/zeppelin-0.7.3/lib/*:/opt/zeppelin-0.7.3/*::/opt/zeppelin-0.7.3/conf org.apache.zeppelin.server.ZeppelinServer\nroot       2621  0.0  0.0 36398804 107228 ?     Sl   20:35   0:05 /usr/bin/java -jar /tmp/moveS3toHDFS.jar\nroot       2622  0.0  0.0   4380   668 ?        S    20:35   0:00 sleep infinity\nroot       2740  0.0  0.0  18104  2904 ?        S    20:51   0:00 /bin/bash /opt/zeppelin-0.7.3/bin/interpreter.sh -d /opt/zeppelin-0.7.3/interpreter/shell -p 40991 -l /opt/zeppelin-0.7.3/local-repo/2D1585T2A\nroot       2751  0.0  0.0  18104  2148 ?        S    20:51   0:00 /bin/bash /opt/zeppelin-0.7.3/bin/interpreter.sh -d /opt/zeppelin-0.7.3/interpreter/shell -p 40991 -l /opt/zeppelin-0.7.3/local-repo/2D1585T2A\nroot       2752  0.0  0.1 7810128 240808 ?      Sl   20:51   0:06 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///opt/zeppelin-0.7.3/conf/log4j.properties -Dzeppelin.log.file=/opt/zeppelin-0.7.3/logs/zeppelin-interpreter-shell--spark-hdfs-adapter.log -Xms1024m -Xmx1024m -XX:MaxPermSize=512m -cp :/opt/zeppelin-0.7.3/interpreter/shell/*:/opt/zeppelin-0.7.3/lib/interpreter/*: org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer 40991\nroot       2948  0.0  0.0  18112  2972 ?        S    20:57   0:00 /bin/bash /opt/zeppelin-0.7.3/bin/interpreter.sh -d /opt/zeppelin-0.7.3/interpreter/spark -p 43704 -l /opt/zeppelin-0.7.3/local-repo/2D37SZR3J\nroot       2960  0.0  0.0  18112  1672 ?        S    20:57   0:00 /bin/bash /opt/zeppelin-0.7.3/bin/interpreter.sh -d /opt/zeppelin-0.7.3/interpreter/spark -p 43704 -l /opt/zeppelin-0.7.3/local-repo/2D37SZR3J\nroot       2961  0.8  0.6 8282444 838000 ?      Sl   20:57   1:04 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -cp /opt/zeppelin-0.7.3/local-repo/2D37SZR3J/*:/opt/zeppelin-0.7.3/interpreter/spark/*:/opt/zeppelin-0.7.3/lib/interpreter/*:/opt/zeppelin-0.7.3/interpreter/spark/zeppelin-spark_2.11-0.7.3.jar:/usr/local/spark/conf/:/usr/local/spark/jars/*:/usr/local/hadoop/etc/hadoop/:/usr/local/hadoop/etc/hadoop/:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs/:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/contrib/capacity-scheduler/*.jar -Xmx1g -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///opt/zeppelin-0.7.3/conf/log4j.properties -Dzeppelin.log.file=/opt/zeppelin-0.7.3/logs/zeppelin-interpreter-spark--spark-hdfs-adapter.log org.apache.spark.deploy.SparkSubmit --conf spark.driver.extraClassPath=:/opt/zeppelin-0.7.3/local-repo/2D37SZR3J/*:/opt/zeppelin-0.7.3/interpreter/spark/*:/opt/zeppelin-0.7.3/lib/interpreter/*::/opt/zeppelin-0.7.3/interpreter/spark/zeppelin-spark_2.11-0.7.3.jar --conf spark.driver.extraJavaOptions= -Dfile.encoding=UTF-8 -Dlog4j.configuration=file:///opt/zeppelin-0.7.3/conf/log4j.properties -Dzeppelin.log.file=/opt/zeppelin-0.7.3/logs/zeppelin-interpreter-spark--spark-hdfs-adapter.log --class org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer /opt/zeppelin-0.7.3/interpreter/spark/zeppelin-spark_2.11-0.7.3.jar 43704\nroot       3904  0.0  0.0  34424  2836 ?        R    22:59   0:00 ps xau\n"}]}},{"text":"%sh\n","user":"anonymous","dateUpdated":"2017-12-20T22:58:37+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1513810717375_-287259046","id":"20171220-225837_164633910","dateCreated":"2017-12-20T22:58:37+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18962"}],"name":"Anomaly detection for improved Quality Management","id":"2D4K7M17P","angularObjects":{"2D37SZR3J:shared_process":[],"2D188RQ8X:shared_process":[],"2D1585T2A:shared_process":[],"2D1W4J5J5:shared_process":[],"2D31GM97H:shared_process":[],"2D46RZVNR:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}